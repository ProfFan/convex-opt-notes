[["index.html", "Convex Optimization Notes Preface", " Convex Optimization Notes Fan Jiang 2021-02-02 Preface This is my notes while studying Prof. Stephen Boyd’s convex optimization course (link). I will try my best covering his points and add figures and code that illustrates his ideas. You will need the following tools to compile it. The bookdown and reticulate package can be installed from CRAN: install.packages(&quot;bookdown&quot;) install.packages(&quot;reticulate&quot;) # for Python snippets To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. "],["intro.html", "Chapter 1 Introduction 1.1 Notations 1.2 Generalized inequalities 1.3 Minimum and minimal elements 1.4 Separating hyperplane theorem 1.5 Dual cones and generalized inequalities 1.6 Minimum and minimal elements via dual inequalities", " Chapter 1 Introduction 1.1 Notations \\(\\inf\\) and \\(\\sup\\): similar to \\(\\min\\) and \\(\\max\\). 1.2 Generalized inequalities The generalized inequality (defined by a proper cone \\(K\\)) is written as \\[\\begin{equation} \\tag{1.1} x\\preceq_{K} y \\Longleftrightarrow y-x \\in K \\end{equation}\\] The componentwise inequality (\\(K=R^n_+\\)) is written as \\[\\begin{equation} \\tag{1.2} x\\preceq_{R_+^n} y \\Longleftrightarrow x_i \\leq y_i \\end{equation}\\] The matrix inequality (\\(K=S^n_+\\)) is written as \\[\\begin{equation} \\tag{1.3} X\\preceq_{S_+^n} Y \\Longleftrightarrow Y - X \\text{ positive definite } \\end{equation}\\] 1.3 Minimum and minimal elements Definition 1.1 \\(x\\in S\\) is the minimum element of \\(S\\) with respect to \\(\\preceq_K\\) if \\[\\begin{equation} \\tag{1.4} y \\in S \\Longleftrightarrow x \\preceq_K y \\end{equation}\\] Definition 1.2 \\(x\\in S\\) is a minimal element of \\(S\\) with respect to \\(\\preceq_K\\) if \\[\\begin{equation} \\tag{1.5} y \\in S, y \\preceq_K x \\Longrightarrow y = x \\end{equation}\\] Minimum: all points are more Minimal: no points are less 1.4 Separating hyperplane theorem Theorem 1.1 if \\(C, D\\) are disjoint convex sets, then there exists \\(a\\neq 0\\) \\[\\begin{equation} \\tag{1.6} a^Tx\\leq b\\text{ for } x\\in C, a^T x \\geq b \\text{ for } x\\in D \\end{equation}\\] Note that this is not strict (both inequalities includes equality). 1.4.1 Supporting hyperplane theorem Definition 1.3 supporting hyperplane to set \\(C\\) at boundary point \\(x_0\\): \\[\\begin{equation} \\tag{1.7} \\{ x | a^T x = a^T x_0 \\} \\end{equation}\\] where \\(a \\neq 0\\) and \\(a^Tx \\leq a^T x_0\\) for all \\(x\\in C\\) Theorem 1.2 If \\(C\\) is convex, then there exists a supporting hyperplane at every boundary point of \\(C\\). 1.5 Dual cones and generalized inequalities Definition 1.4 dual cone of a cone \\(K\\) \\[\\begin{equation} \\tag{1.8} K^* = \\{ y | y^T x \\geq 0 \\text{ for all } x\\in K\\} \\end{equation}\\] Examples: \\(R^n_+\\): \\(R^n_+\\) \\(K = S_+^n\\): \\(S_+^n\\) (\\(&lt;x,y&gt; = Tr(XY)\\)) \\(K = \\{ (x, t) | \\| x \\|_2 \\leq t \\}\\): \\(K = \\{ (x, t) | \\| x \\|_2 \\leq t \\}\\) \\(K = \\{ (x, t) | \\| x \\|_1 \\leq t \\}\\): \\(K = \\{ (x, t) | \\| x \\|_\\infty \\leq t \\}\\) Note that \\((K^*)^* = K\\) only if \\(K\\) is proper. Also, since dual cones of proper cones are proper, this defines generalized inequalities: \\[\\begin{equation*} y \\succeq_K^* 0 \\Longleftrightarrow y^Tx \\geq 0 \\text{ for all } x\\succeq_K 0 \\end{equation*}\\] 1.6 Minimum and minimal elements via dual inequalities Theorem 1.3 \\(x\\in S\\) is the minimum element of \\(S\\) with respect to \\(\\preceq_K\\) iff for all \\(\\lambda \\succ_{K^*} 0\\), \\(x\\) is the unique minimizer of \\(\\lambda^T z\\) over \\(S\\). Theorem 1.4 \\(x\\in S\\) is a minimal element of \\(S\\) with respect to \\(\\preceq_K\\) if \\(x\\) minimizes \\(\\lambda^Tz\\) over \\(S\\) for some \\(\\lambda \\succ_{K^*} 0\\) "],["convex-functions.html", "Chapter 2 Convex functions 2.1 Definition 2.2 Examples 2.3 Examples on \\(R^n\\) and \\(R^{m\\times n}\\) 2.4 Restriction of a convex function to a line 2.5 Extension of a function outside of its domain 2.6 1st-order condition 2.7 2nd-order conditions 2.8 Examples 2.9 Epigraph and sublevel set 2.10 Jensen’s inequality 2.11 Positive weighted sum and composition with affine functions 2.12 Pointwise maximum 2.13 Pointwise supremum 2.14 Composition 2.15 Vector Composition 2.16 Pointwise infinium 2.17 Perspective function 2.18 The conjugate function 2.19 Quasiconvexity 2.20 Properties of quasiconvex functions 2.21 Log-concavity 2.22 Generic inequalities", " Chapter 2 Convex functions 2.1 Definition Definition 2.1 \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) is convex if \\(\\mathbf{dom} f\\) is a convex set and \\[\\begin{equation} \\tag{2.1} f(\\underbrace{\\theta x + (1-\\theta) y}_\\text{convex mixture}) \\leq \\theta f(x) + (1-\\theta) f(y) \\end{equation}\\] for all \\(x, y \\in \\mathbf{dom} f\\), \\(0\\leq \\theta \\leq 1\\). i.e. “Any chord is above function.” \\(f\\) is concave if \\(-f\\) is convex. 2.2 Examples convex: affine functions \\(ax+b\\) exponential \\(e^{ax}\\) powers: \\(x^a\\) on \\(R_{++}\\), for \\(a\\geq 1\\) or \\(a\\leq 0\\) powers of absolute value: \\(| x |^p\\) on \\(R\\), for \\(p\\geq 1\\) negative entropy: \\(x \\log x\\) on \\(R_{++}\\) concave: affine functions \\(ax+b\\) (lol!) powers: \\(x^a\\) on \\(R_{++}\\), for \\(0 \\leq a \\leq 1\\) log: \\(\\log x\\) on \\(R_{++}\\) 2.3 Examples on \\(R^n\\) and \\(R^{m\\times n}\\) Affine functions are both convex and concave; all norms are convex. 2.3.1 On \\(R^n\\) affine: \\(a^Tx+b\\) norms: \\(\\| x \\|_p = (\\sum_{i=1}^n | x_i | ^p )^{1/p}\\) for \\(p\\geq 1\\); \\(\\| x\\|_\\infty = max_k |x_k|\\) 2.3.2 On \\(R^{m\\times n}\\) (matrices) Affine: \\[\\begin{align} \\tag{2.2} f(X) &amp; = &amp; \\langle A, X \\rangle + b \\\\ &amp; = &amp; \\mathbf{tr}(A^TX) + b \\end{align}\\] Spectral norm (maximum singular value norm): \\[\\begin{align} \\tag{2.3} f(X)= \\| X \\|_2 = \\sigma_{\\mathrm{max}}(X) = (\\lambda_{\\mathrm{max}}(X^TX))^{1/2} \\end{align}\\] 2.4 Restriction of a convex function to a line Theorem 2.1 \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) is convex iff the function \\(g: \\mathbf R \\rightarrow \\mathbf R\\) \\[\\begin{equation*} g(t) = f(x + tv), \\mathbf{dom} g = \\{ t | x + tv \\in \\mathbf{dom} f \\} \\end{equation*}\\] is convex in \\(t\\) for any \\(x \\in \\mathbf{dom} f\\), \\(v \\in \\mathbf R^n\\). This fact (2.1) is very important when speculating :) and proving whether a function is convex or concave. 2.5 Extension of a function outside of its domain extended-value extension \\(\\bar f\\) of \\(f\\) is \\[\\begin{align} \\tag{2.4} \\bar f = f(x), x\\in \\mathbf{dom} f\\\\ \\bar f = \\infty, x \\notin \\mathbf{dom} f \\end{align}\\] This usually simplifies the notations when doing analysis. 2.6 1st-order condition Theorem 2.2 differentiable \\(f\\) with convex domain is convex iff \\[\\begin{equation*} f(y) \\geq f(x) + \\nabla f(x)^T (y-x) \\text{ for all } x,y \\in \\mathbf{dom} f \\end{equation*}\\] i.e. the 1st-order Taylor expansion (local) is the global under-estimator for \\(f\\). 2.7 2nd-order conditions Theorem 2.3 twice differentiable \\(f\\) with convex domain is convex iff \\[\\begin{equation*} \\nabla^2 f(x) \\succeq 0 \\text{ for all } x \\in \\mathbf{dom} f. \\end{equation*}\\] If (not iff) \\(\\nabla^2 f(x) \\succ 0\\), then \\(f\\) is strictly convex. 2.8 Examples Example 2.1 quadratic function: \\(f(x) = (1/2)x^TPx+ q^T x + r\\) (with \\(P\\in S^n\\)) \\[\\begin{equation*} \\nabla f(x) = Px+q, \\nabla^2 f(x) = P \\end{equation*}\\] convex if \\(P\\succeq 0\\). Example 2.2 least squares objective: \\(f(x) = \\|Ax-b\\|^2_2\\) \\[\\begin{equation*} \\nabla f(x) = 2A^T(Ax-b), \\nabla^2 f(x) = 2A^TA, \\end{equation*}\\] is convex. Example 2.3 quadratic-over-linear: \\(f(x,y) = x^2/y\\) \\[\\begin{equation*} \\nabla^2 f(x,y) = \\frac 2 {y^3}\\begin{bmatrix} y \\\\ -x \\end{bmatrix} \\begin{bmatrix} y \\\\ -x \\end{bmatrix}^T \\succeq 0, \\end{equation*}\\] is convex if \\(y &gt; 0\\). Example 2.4 log-sum-exp (softmax): \\(f(x) = \\log \\sum^n_{k=1} e^{x_k}\\), \\(z_k = \\exp x_k\\) \\[\\begin{equation*} \\nabla^2 f(x) = \\text{&lt;horrible mess&gt;} = \\frac 1 {\\mathbf{1}^{T}z} \\mathbf{diag}(z) - \\frac 1 {(\\mathbf{1}^T z)^2}zz^T, \\end{equation*}\\] is convex. Proof is omitted, but very important! 2.9 Epigraph and sublevel set Definition 2.2 \\(\\alpha\\)-sublevel set of \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\): \\[\\begin{equation*} C_\\alpha = \\{ x\\in \\mathbf{dom} f | f(x) \\leq \\alpha \\} \\end{equation*}\\] Sublevel sets of convex functions are convex (but not in reverse). Definition 2.3 epigraph of \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\): \\[\\begin{equation*} \\mathbf{epi} f = \\{ (x,t) \\in R^{n+1} | x\\in \\mathbf{dom} f, f(x) \\leq t \\} \\end{equation*}\\] \\(f\\) is convex iff \\(\\mathbf{epi} f\\) is a convex set. 2.10 Jensen’s inequality An extension to (2.1) is the Jensen’s inequality for expectations: \\[\\begin{equation} \\tag{2.5} f(\\mathbf E z) \\leq \\mathbf E f(z) \\end{equation}\\] for any random variable \\(z\\). For a discrete distribution, (2.1) reduces to equality \\(P(B) = 1 - P(A)\\). 2.11 Positive weighted sum and composition with affine functions Note that if \\(f\\) is convex, \\(f(Ax+b)\\) is also convex. Examples: log barrier: \\[\\begin{equation*} f(x) = - \\sum^m \\log(b_i - a_i^Tx), \\mathbf{dom} f = \\{ x| a_i^Tx &lt; b_i\\} \\end{equation*}\\] 2.12 Pointwise maximum Proposition 2.1 if \\(f_1,\\dots, f_m\\) are convex, then \\(f(x) = max\\{f_1(x), \\dots, f_m(x)\\}\\) is convex. Example 2.5 piecewise-linear: \\(\\max_{i=1,\\dots,m}(a_i^Tx+b_i)\\) is convex. Example 2.6 sum of \\(r\\) largest components of \\(x\\in R^n\\): \\[\\begin{equation*} f(x) = x_{[1]} + \\cdots + x_{[r]} \\end{equation*}\\] is convex. Note: \\(f(x)\\) can be seen as the max of \\(C_n^r\\) sums. 2.13 Pointwise supremum Proposition 2.2 if \\(f(x,y)\\) is convex in \\(x\\) for each \\(y \\in \\mathcal A\\), then \\[\\begin{equation} \\tag{2.6} g(x) = \\sup_{y\\in \\mathcal A} f(x, y) \\end{equation}\\] is convex. Example 2.7 support function of a set \\(C\\): \\(S_C(x) = \\sup_{y\\in C}y^Tx\\) Example 2.8 max eigenvalue of symmetric matrix: for \\(X\\in S^n\\), \\[\\begin{equation} \\tag{2.7} \\lambda_{max}(X) = \\sup_{\\|y\\|_2=1}\\underbrace{y^TXy}_{\\text{linear in }X} \\end{equation}\\] is convex. 2.14 Composition \\[\\begin{equation} \\tag{2.8} f(x) = h(g(x)) \\end{equation}\\] \\(f\\) is convex if \\(g\\) convex, \\(\\tilde h\\) convex and nondecreasing \\(g\\) concave, \\(\\tilde h\\) convex and nonincreasing Note that \\(\\tilde h\\) is the extended value function so be careful! 2.15 Vector Composition Similar to 2.14 2.16 Pointwise infinium Proposition 2.3 if \\(f(x,y)\\) is convex in \\((x,y)\\) and \\(C\\) is a convex set, then \\[\\begin{equation} \\tag{2.9} g(x) = \\inf_{y\\in \\mathcal C} f(x, y) \\end{equation}\\] is convex. Example 2.9 Schur complement \\(f(x,y) = x^TAx + 2x^TBy + y^TCy\\) with \\[\\begin{equation} \\tag{2.10} \\begin{bmatrix} A &amp; B \\\\ B^T &amp; C \\end{bmatrix} \\succeq 0, C \\succ 0 \\end{equation}\\] minimizing over \\(y\\) gives \\(g(x) = \\inf_y f(x,y) = x^T(A-BC^{-1}B^T)x\\) (Schur complement). \\(g\\) is convex, thus the Schur complement \\(A-BC^{-1}B^T \\succeq 0\\). Example 2.10 distance to a set \\[\\begin{equation} \\tag{2.11} \\mathbf{dist}(x, S) = \\inf_{y\\in S} \\|x-y\\| \\end{equation}\\] is convex if \\(S\\) is convex. 2.17 Perspective function Definition 2.4 the perspective of a function \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) is the function \\(g: \\mathbf R^n \\times R \\rightarrow \\mathbf R\\), \\[\\begin{equation} g(x,t) = tf(x/t), \\mathbf{dom} g = \\{ (x,t) | x/t \\in \\mathbf{dom} f, t&gt;0 \\} \\end{equation}\\] \\(g\\) is convex if f is convex. Example 2.11 \\(f(x) = x^Tx\\) is convex, then \\(g(x, t) = x^Tx/t\\) is convex for \\(t&gt;0\\). Example 2.12 negative log \\(f(x) = -\\log x\\) is convex, thus the relative entropy \\[\\begin{equation} \\tag{2.12} g(x,t) = t\\log t - t \\log x \\end{equation}\\] is convex on \\(\\mathbf R^2_{++}\\). (This is related to KL divergence) Example 2.13 if \\(f\\) convex, then \\[\\begin{equation} \\tag{2.13} g(x) = (c^Tx+d) f \\left ( (Ax+b)/(c^Tx+d) \\right ) \\end{equation}\\] is convex on \\(\\{ x | c^Tx+d &gt; 0 , (Ax+b)/(c^Tx+d) \\in \\mathbf{dom} f \\}\\) 2.18 The conjugate function Definition 2.5 the conjugate of a function \\(f\\) is \\[\\begin{equation} \\tag{2.14} f^*(y) = \\sup_{x\\in \\mathbf{dom} f}(y^Tx-f(x)) \\end{equation}\\] Note that \\(f^*\\) is convex regardless of what \\(f\\) is. And an interesting fact is that \\[\\begin{equation} \\mathbf{epi} (f^{env}) = \\mathbf{conv}(\\mathbf{epi} f) \\end{equation}\\] (convex envelope) 2.19 Quasiconvexity Definition 2.6 \\(f: R^n \\rightarrow R\\) is quasiconvex (unimodal) if \\(\\mathbf{dom} f\\) is convex and the sublevel sets \\[\\begin{equation} \\tag{2.15} S_\\alpha = \\{ x\\in \\mathbf{dom} f | f(x) \\leq \\alpha \\} \\end{equation}\\] are convex for all \\(\\alpha\\). \\(f\\) is quasiconcave if \\(-f\\) is quasiconvex \\(f\\) is quasilinear if it is both quasiconvex and quasiconcave 2.19.1 Examples \\(\\sqrt{|x|}\\) is quasiconvex on \\(R\\) \\(\\mathrm{ceil}(x) = \\inf \\{ z\\in Z | z \\geq x \\}\\) is quasiconvex \\(\\log x\\) is quasilinear on \\(R_{++}\\) \\(f(x_1, x_2) = x_1 x_2\\) is quasiconcave on \\(R^2_{++}\\) linear fractional functions are quasilinear (!) distance ratio \\[\\begin{equation} \\tag{2.16} f(x) = \\frac{\\| x-a \\|_2}{\\| x-b \\|_2}, \\mathrm{dom}f=\\{ x| \\|x-a\\|_2 \\leq \\| x-b \\|_2 \\} \\end{equation}\\] is quasiconvex Note that the sum of two quasiconvex functions are generally not quasiconvex. 2.19.1.1 Internal rate of return (IRR) cash flow \\(x=(x_0,\\dots, x_n)\\), \\(x_i\\) is payment in period \\(i\\) assume \\(x_0 &lt; 0\\) and \\(\\sum x_n &gt; 0\\) present value of cash flow \\(x\\) for interest rate \\(r\\): \\[\\begin{equation} \\tag{2.17} \\mathrm{PV}(x,r) = \\sum_{i=0}^{n} (1+r)^{-i}x_i \\end{equation}\\] IRR is smallest interest rate for which \\(PV(x,r) = 0\\): \\[\\begin{equation} \\tag{2.18} \\mathrm{IRR}(x) = \\inf \\{r \\geq 0 | PV(x,r) = 0\\} \\end{equation}\\] IRR is quasiconcave: superlevel set is intersection of halfspaces \\[\\begin{equation} \\tag{2.19} \\mathrm{IRR}(x)\\geq R \\Longleftrightarrow \\mathrm{PV}(x,r) \\geq 0\\text{ for } 0\\leq r \\leq R \\end{equation}\\] 2.20 Properties of quasiconvex functions Theorem 2.4 Jensen’s inequality for quasiconvex functions if \\(f\\) quasiconvex, then \\[\\begin{equation} \\tag{2.20} 0\\leq \\theta \\leq 1 \\Longrightarrow f(\\theta x + (1- \\theta)y) \\leq \\max \\{f(x), f(y) \\} \\end{equation}\\] This fact is shown in (2.1). Figure 2.1: Jensen’s inequality for quasiconvex functions Theorem 2.5 first order condition: differentiable \\(f\\) with convex domain is quasiconvex iff \\[\\begin{equation} \\tag{2.21} f(y)\\leq f(x) \\Longrightarrow \\nabla f(x)^T (y-x) \\leq 0 \\end{equation}\\] 2.21 Log-concavity \\(\\log f\\) is concave many probability densities are log-concave, e.g. the Gaussian \\[\\begin{equation} \\tag{2.22} \\frac 1 {\\sqrt{(2\\pi)^n \\det \\Sigma}} \\exp (-\\frac 1 2 (x-\\bar x)^T\\Sigma^{-1}(x-\\bar x)) \\end{equation}\\] cumulative Gaussian \\(\\Phi\\) is log-concave \\[\\begin{equation} \\tag{2.23} \\Phi (x) = \\frac 1 {\\sqrt{2\\pi}} \\int_{-\\infty}^x \\exp(-u^2/2) du \\end{equation}\\] 2.21.1 Properties Proposition 2.4 twice differentiable \\(f\\) with convex domain is log-concave iff \\[\\begin{equation} \\tag{2.24} \\nabla^2f(x) \\preceq \\frac {\\nabla f(x) \\nabla f(x)^T}{f(x)} \\end{equation}\\] (2.4) basically says that you can have 1 positive eigenvalue for a log-concave function. product of log-concave is still log-concave sum, not really integration: yes, see following (2.5) Proposition 2.5 if \\(f: R^n \\times R^m \\rightarrow R\\) is log-concave, then \\[\\begin{equation*} g(x) = \\int f(x,y) dy \\end{equation*}\\] is log-concave. b/c (2.5), we have the follow interesting results: Proposition 2.6 convolution preserves log-concavity: \\[\\begin{equation} \\tag{2.25} (f*g)(x) = \\int f(x-y) g(y) dy \\end{equation}\\] is log-concave. Proposition 2.7 if \\(C\\subseteq R^n\\) convex and \\(y\\) is a random variable with log-concave pdf then \\[\\begin{equation} \\tag{2.26} f(x) = \\mathbf{prob} (x+y\\in C) \\end{equation}\\] is log-concave. Proof. \\[\\begin{equation*} f(x) = \\int g(x+y)p(y)dy, g(u) = \\begin{cases}1 &amp; u\\in C \\\\ 0 &amp; u\\notin C \\end{cases} \\end{equation*}\\] \\(p\\) is pdf of \\(y\\). Note that 2.7 directly relates to the yield optimization problem in operational research. 2.22 Generic inequalities Definition 2.7 \\(f: \\mathbf R^n \\rightarrow \\mathbf R^m\\) is \\(K\\)-convex if \\(\\mathbf{dom} f\\) is a convex set and \\[\\begin{equation} \\tag{2.27} f(\\theta x + (1-\\theta) y) \\preceq_K \\theta f(x) + (1-\\theta) f(y) \\end{equation}\\] for all \\(x, y \\in \\mathbf{dom} f\\), \\(0\\leq \\theta \\leq 1\\). One particularly interesting case is for matrices: \\(f: \\mathbf S^n \\rightarrow \\mathbf S^m\\). Example 2.14 \\(f(X) = X^2\\) is \\(S_+^m\\)-convex. "],["convex-optimization.html", "Chapter 3 Convex Optimization 3.1 Optimization in standard form 3.2 Optimality and local optimality 3.3 Implicit constraints 3.4 Feasibility problem 3.5 Convex optimization 3.6 Optimality in convex optimization 3.7 Equivalent convex problems 3.8 Dummy", " Chapter 3 Convex Optimization 3.1 Optimization in standard form \\[\\begin{align} \\tag{3.1} \\text{minimize}&amp; &amp; f_0(x) &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_i(x) \\leq 0 &amp; , &amp; i=1,\\dots,m \\\\ &amp; &amp; h_i(x) = 0 &amp; , &amp; i=1,\\dots,p \\end{align}\\] A standard form of describing optimization problems is (3.1), where \\(x\\in R^n\\) is the optimization variable \\(f_0: R^n \\rightarrow R\\) is the objective or cost function \\(f_i: R^n \\rightarrow R\\) are the inequality constraints \\(h_i: R^n \\rightarrow R\\) are the equality constraints For such a description, the optimal value is defined as \\[\\begin{equation} \\tag{3.2} p^* = \\inf \\{f_0(x) | f_i(x)\\leq 0, h_i(x) = 0 \\}. \\end{equation}\\] When the problem is infeasible, \\(p^*=\\infty\\), when it’s unbounded below, \\(p^* = -\\infty\\). 3.2 Optimality and local optimality feasible: in domain and satisfy all constraints optimal: \\(f_0(x) = p^*\\) locally optimal: exists \\(R&gt;0\\) that \\(x\\) is optimal under \\(\\| z-x \\|_2 \\leq R\\). 3.3 Implicit constraints ((3.1)) has an implicit constraint \\[\\begin{equation} \\tag{3.3} x\\in \\mathcal D = \\bigcap_{i=0}^m \\mathbf{dom} f_i \\cup \\bigcap_{i=1}^p \\mathbf{dom} h_i \\end{equation}\\] \\(\\mathcal D\\) is called the domain of the problem a problem is unconstrained if it has no explict constraints (\\(m=p=0\\)) Example 3.1 \\[\\begin{align} \\text{minimize} &amp; &amp; f_0 = -\\sum_{i=1}^k \\log (b_i-a_i^Tx) \\end{align}\\] is an unconstrained problem with implicit constraint of \\(a_i^Tx&lt;b_i\\). 3.4 Feasibility problem \\[\\begin{align} \\tag{3.4} \\text{find}&amp; &amp; x &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_i(x) \\leq 0 &amp; , &amp; i=1,\\dots,m \\\\ &amp; &amp; h_i(x) = 0 &amp; , &amp; i=1,\\dots,p \\end{align}\\] can be actually written as \\[\\begin{align} \\tag{3.5} \\text{minimize}&amp; &amp; 0 &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_i(x) \\leq 0 &amp; , &amp; i=1,\\dots,m \\\\ &amp; &amp; h_i(x) = 0 &amp; , &amp; i=1,\\dots,p \\end{align}\\] \\(p^*=0\\) if constraints are feasible \\(p^*=\\infty\\) otherwise (\\(\\inf \\varnothing = \\infty\\)) 3.5 Convex optimization \\[\\begin{align} \\tag{3.6} \\text{minimize}&amp; &amp; f_0(x) &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_i(x) \\leq 0 &amp; , &amp; i=1,\\dots,m \\\\ &amp; &amp; Ax=b &amp; &amp; \\end{align}\\] Boyd defines convex optimization to be: \\(f_i\\) are all convex, equality constraints are all affine. also can define quasiconvex problems where \\(f_0\\) is quasiconvex, and \\(f_i (i&gt;0)\\) are convex. Note that the feasible set of a convex optimization problem is convex. Example 3.2 \\[\\begin{align} \\text{minimize}&amp; &amp; f_0(x)=x_1^2+x_2^2 &amp; \\\\ \\text{subject to} &amp; &amp; f_i(x)=x_1/(1+x_2^2) \\leq 0 &amp; \\\\ &amp; &amp; h_1(x) = (x_1 + x_2)^2=0 &amp; \\end{align}\\] \\(f_0\\) is convex, feasible set \\(\\{ x_1 = -x_2 \\}\\) is also convex however not a convex problem according to (3.6) we can rewrite to equivalent (not identical) convex problem \\[\\begin{align} \\text{minimize}&amp; &amp; f_0(x)=x_1^2+x_2^2 &amp; \\\\ \\text{subject to} &amp; &amp; x_1 \\leq 0 &amp; \\\\ &amp; &amp; h_1(x) = x_1 + x_2=0 &amp; \\end{align}\\] 3.6 Optimality in convex optimization Theorem 3.1 Any local optimal point is optimal globally for a convex problem. One can easily prove (3.1) by contradiction. 3.6.1 Optimality criterion for differentiable \\(f_0\\) Theorem 3.2 \\(x\\) is optimal iff \\(x\\) is feasible and \\[\\begin{align} \\tag{3.7} \\nabla f(x)^T (y-x) \\geq 0 &amp; \\text{ for all feasible }y \\end{align}\\] We have seen this before in 2.2. Example 3.3 unconstrained problem: \\(\\nabla f(x) = 0\\) Example 3.4 equality constrained problem (only \\(Ax=b\\)): \\(x\\) is optimal iff there exists \\(\\nu\\) such that \\(x\\in \\mathbf{dom} f_0\\), \\(Ax=b\\), \\(\\nabla f_0(x) + A^T\\nu=0\\) Proof. Assume \\(x\\) is the optimum. \\(\\nabla f_0(x)^T (z-x) \\geq 0\\) for all \\(z\\), \\(Az=b\\). Also \\(Ax=b\\). Thus \\(z-x \\in \\mathrm{Null}(A)\\), i.e. \\(\\nabla f_0(x) \\perp \\mathrm{Null}(A)\\). Consequently, \\(f_0(x) \\in \\operatorname{im} (A^T)\\), Thus \\(\\nabla f_0(x) = A^Tu\\), \\(\\nabla f_0(x) + A^T(-u) = 0\\). Example 3.5 minimization over non-negative orthant (\\(R_{++}^n\\)): \\[\\begin{align} \\text{minimize}&amp; &amp; f_0(x) &amp; \\\\ \\text{subject to} &amp; &amp; x \\succeq 0 &amp; \\end{align}\\] \\(x\\) is optimal iff \\[\\begin{align} \\end{align}\\] Proof. Assume \\(x\\) is the optimum. \\(\\nabla f_0(x)^T (z-x) \\geq 0\\) for all \\(z\\geq 0\\). Plugging in \\(z=0\\), \\(\\nabla f(x)^Tx \\leq 0\\). If any of the components (\\(i\\)-th) of \\(\\nabla f(x)\\) is negative, then we can take \\(z_i\\rightarrow \\infty\\), and LHS will be negative, thus we know that \\(\\nabla f(x) \\geq 0\\). Since \\(\\nabla f(x)^Tx \\leq 0\\) and \\(\\nabla f(x) \\geq 0\\), we know that \\(\\nabla f(x)_i x_i = 0\\) for all \\(i\\) (complementarity). 3.7 Equivalent convex problems This section is full of trickery that you will regret if you don’t know. 3.7.1 eliminating equality constraints \\[\\begin{align} \\text{minimize}&amp; &amp; f_0(x) &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_i(x) \\leq 0 &amp; , &amp; i=1,\\dots,m \\\\ &amp; &amp; Ax=b &amp; &amp; \\end{align}\\] is equivalent to \\[\\begin{align} \\tag{3.6} \\text{minimize (over z)}&amp; &amp; f_0(Fz+x_0) &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_i(Fz+x_0) \\leq 0 &amp; , &amp; i=1,\\dots,m \\\\ \\end{align}\\] where \\(F\\) and \\(x_0\\) are such that \\[\\begin{align} \\tag{3.8} Ax=b &amp; \\Longleftrightarrow &amp; x=Fz+x_0 \\text{ for some }z \\end{align}\\] How to get \\(F\\) and \\(x_0\\)? Well \\(F\\) spans the null space of \\(A\\), and \\(x_0\\) is any particular solution of \\(Ax=b\\). Note affine operation preserves convexity, so the eliminated problem is also convex. Boyd: An intriguing fact is that in reality not only it’s bad to eliminate constraints, it’s often a good idea to add (un-eliminate) equality constraints! Boyd: Convex problems (by definition here) only has affine equality constraints. 3.7.2 Introducing equality constraints \\[\\begin{align} \\text{minimize}&amp; &amp; f_0(A_0x+b_0) &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_i(A_0x+b_0) \\leq 0 &amp; , &amp; i=1,\\dots,m \\\\ \\end{align}\\] is equivalent to \\[\\begin{align} \\text{minimize over $x$, $y_i$}&amp; &amp; f_0(y_0) &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_i(y_i) \\leq 0 &amp; , &amp; i=1,\\dots,m \\\\ &amp; &amp; y_i=A_ix+b_i &amp; &amp; i=1,\\dots,m \\end{align}\\] 3.7.3 Introducing slack variables for linear inequalities \\[\\begin{align} \\text{minimize}&amp; &amp; f_0(x) &amp; &amp; \\\\ \\text{subject to} &amp; &amp; a_i^Tx \\leq b_i &amp; , &amp; i=1,\\dots,m \\\\ \\end{align}\\] is equivalent to is equivalent to \\[\\begin{align} \\text{minimize over $x$, $s$}&amp; &amp; f_0(x) &amp; &amp; \\\\ \\text{subject to} &amp; &amp; a_i^Tx+s_i=b_i &amp; , &amp; i=1,\\dots,m \\\\ &amp; &amp; -s_i \\leq 0 &amp; &amp; i=1,\\dots,m \\end{align}\\] Boyd: this looks stupid but is actually useful. 3.7.4 The epigraph form (trick) The standard form (3.6) is equivalent to \\[\\begin{align} \\text{minimize over $x$, $t$}&amp; &amp; t &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_0(x) - t \\leq 0 &amp; &amp; \\\\ &amp; &amp; f_i(x) \\leq 0 &amp; , &amp; i=1,\\dots,m \\\\ &amp; &amp; Ax=b &amp; &amp; \\end{align}\\] Note that this actually converts any objective to a linear objective! 3.7.5 Minimizing over some variables \\[\\begin{align} \\text{minimize}&amp; &amp; f_0(x_1, x_2) &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_i(x_1) \\leq 0 &amp; &amp; i=1,\\dots,m \\end{align}\\] is equivalent to \\[\\begin{align} \\text{minimize}&amp; &amp; \\tilde f_0(x_1) &amp; &amp; \\\\ \\text{subject to} &amp; &amp; f_i(x_1) \\leq 0 &amp; &amp; i=1,\\dots,m \\end{align}\\] where \\(\\tilde f_0 (x_1) = \\inf_{x_2} f_0(x_1, x_2)\\) a.k.a. dynamic programming preserves convexity. 3.8 Dummy Lorem Lorem Lorem Lorem \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) "],["linear-programs.html", "Chapter 4 Linear Programs 4.1 Piecewise linear minimization 4.2 Chebyshev center of a polyhedron", " Chapter 4 Linear Programs Here are some famous LPs: 4.1 Piecewise linear minimization \\[\\begin{align} \\tag{4.1} \\text{minimize}&amp; &amp; \\max _{i=1,\\dots,m}(a_i^Tx+b_i) \\end{align}\\] is equivalent to the LP \\[\\begin{align} \\tag{4.2} \\text{minimize}&amp; &amp; t &amp; &amp; \\\\ \\text{subject to} &amp; &amp; a_i^Tx+b_i \\leq t &amp; , &amp; i=1,\\dots,m. \\end{align}\\] Note that PWL can approximate any function, so this can be very useful! 4.2 Chebyshev center of a polyhedron The Chebyshev center of a polyhedron \\[\\begin{equation} \\tag{4.3} \\mathcal P = \\{ x|a_i^Tx &lt; b_i \\} \\end{equation}\\] is defined to be the center of a maximum sphere \\[\\begin{equation} \\tag{4.4} \\mathcal B = \\{ x_c + u | \\| u \\|_2 \\leq r \\} \\end{equation}\\] inside the polyhedron (note this can be multiple such points). Furthermore, \\(a_i^Tx \\leq b_i\\) for all \\(x \\in \\mathcal B\\) if and only if \\[\\begin{equation} \\tag{4.5} \\sup\\{ a_i^T (x_c + u) | \\| u \\|_2 \\leq r\\} = a_i^T x_c + r \\| a_i \\|_2 \\leq b_i \\end{equation}\\] This is actually an LP: \\[\\begin{align} \\tag{4.6} \\text{maximize}&amp; &amp; r &amp; &amp; \\\\ \\text{subject to} &amp; &amp; a_i^Tx_c + \\|a_i\\|_2 r \\leq b_i, &amp; &amp; i=1,\\dots,m \\end{align}\\] "],["interior-point-methods.html", "Chapter 5 Interior Point Methods 5.1 KKT conditions 5.2 Logarithmic barrier function and central path", " Chapter 5 Interior Point Methods Inequality constrained minimization problems can be written as \\[\\begin{align} \\tag{5.1} \\text{minimize}&amp; &amp; f_0(x) &amp; &amp; \\\\ \\text{subject to}&amp; &amp; f_i(x) \\leq 0, &amp; &amp; i = 1,\\dots,m \\\\ &amp; &amp; Ax=b &amp; \\end{align}\\] where \\(f_0,\\dots,f_m:\\mathbf{R}^n\\rightarrow \\mathbf R^n\\) are convex and \\(\\mathcal{C}^2\\) differentiable, and \\(A\\in \\mathbf R^{p\\times n}\\) with \\(\\mathbf{rank} A = p &lt; n\\) (Boyd and Vandenberghe 2004). Assuming that the solution \\(x^*\\) exists with the optimum value \\(p^* = f_0(x^*)\\). 5.1 KKT conditions \\[\\begin{align} \\tag{5.2} Ax^* = b, f_i(x^*) &amp; \\leq &amp; 0 &amp;, &amp; i=1,\\dots, m \\\\ \\lambda^* &amp; \\leq &amp; 0 &amp; &amp; \\\\ \\nabla f_0(x^*) + \\sum_{i=1}^{m} \\lambda^*_i \\nabla f_i(x^*) + A^Tv^* &amp; = &amp; 0 &amp; &amp; \\\\ \\lambda^*_i f_i(x^*) &amp; = &amp; 0 &amp;, &amp; i=1,\\dots,m \\end{align}\\] The KKT conditions (5.2) means, respectively: All constraints are satisified at \\(x^*\\); Corresponding Lagrange multiplier is sufficiently larger than zero; The gradient of the Lagrangian at \\(x^*\\) is 0; Inequality constraints are either inactive or on the edge. 5.2 Logarithmic barrier function and central path The aim is to approximate (5.1) as an equality-constrained problem and apply Newton’s method. This proposes the following formulation: \\[\\begin{align} \\tag{5.3} \\text{minimize}&amp; &amp; f_0(x) + \\sum_{i=1}^m I_{-}(f_i(x)) \\\\ \\text{subject to}&amp; &amp; Ax = b, \\end{align}\\] where \\(I_{-} : \\mathrm R \\rightarrow \\mathrm R\\) is the indicator function \\[\\begin{equation} \\tag{5.4} I_{-}(u) = \\begin{cases} 0 &amp; u\\leq 0 \\\\ \\infty &amp; u &gt; 0. \\end{cases} \\end{equation}\\] However, it is obvious that (5.3) is non-differentiable, so we cannot optimize it with Newton’s method. 5.2.1 Logarithmic barrier We can replace \\(I_{-}\\) (5.4) with another function: \\[\\begin{equation} \\tag{5.5} \\hat I_{-}(u) = -(1/t) \\log (-u) \\end{equation}\\] Figure 5.1: Logarithmic barrier This will convert (5.3) "],["references.html", "References", " References Boyd, Stephen, and Lieven Vandenberghe. 2004. “Interior-Point Methods.” In Convex Optimization, 561–630. Cambridge University Press. https://doi.org/10.1017/CBO9780511804441.012. "]]
