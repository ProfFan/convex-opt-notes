[["index.html", "Convex Optimization Notes Preface", " Convex Optimization Notes Fan Jiang 2021-01-30 Preface This is my notes while studying Prof. Stephen Boyd’s convex optimization course. You will need the following tools to compile it. The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.org/tinytex/. "],["intro.html", "Chapter 1 Introduction 1.1 Notations 1.2 Generalized inequalities 1.3 Minimum and minimal elements 1.4 Separating hyperplane theorem 1.5 Dual cones and generalized inequalities 1.6 Minimum and minimal elements via dual inequalities", " Chapter 1 Introduction 1.1 Notations \\(\\inf\\) and \\(\\sup\\): similar to \\(\\min\\) and \\(\\max\\). 1.2 Generalized inequalities The generalized inequality (defined by a proper cone \\(K\\)) is written as \\[\\begin{equation} \\tag{1.1} x\\preceq_{K} y \\Longleftrightarrow y-x \\in K \\end{equation}\\] The componentwise inequality (\\(K=R^n_+\\)) is written as \\[\\begin{equation} \\tag{1.2} x\\preceq_{R_+^n} y \\Longleftrightarrow x_i \\leq y_i \\end{equation}\\] The matrix inequality (\\(K=S^n_+\\)) is written as \\[\\begin{equation} \\tag{1.3} X\\preceq_{S_+^n} Y \\Longleftrightarrow Y - X \\text{ positive definite } \\end{equation}\\] 1.3 Minimum and minimal elements Definition 1.1 \\(x\\in S\\) is the minimum element of \\(S\\) with respect to \\(\\preceq_K\\) if \\[\\begin{equation} \\tag{1.4} y \\in S \\Longleftrightarrow x \\preceq_K y \\end{equation}\\] Definition 1.2 \\(x\\in S\\) is a minimal element of \\(S\\) with respect to \\(\\preceq_K\\) if \\[\\begin{equation} \\tag{1.5} y \\in S, y \\preceq_K x \\Longrightarrow y = x \\end{equation}\\] Minimum: all points are more Minimal: no points are less 1.4 Separating hyperplane theorem Theorem 1.1 if \\(C, D\\) are disjoint convex sets, then there exists \\(a\\neq 0\\) \\[\\begin{equation} \\tag{1.6} a^Tx\\leq b\\text{ for } x\\in C, a^T x \\geq b \\text{ for } x\\in D \\end{equation}\\] Note that this is not strict (both inequalities includes equality). 1.4.1 Supporting hyperplane theorem Definition 1.3 supporting hyperplane to set \\(C\\) at boundary point \\(x_0\\): \\[\\begin{equation} \\tag{1.7} \\{ x | a^T x = a^T x_0 \\} \\end{equation}\\] where \\(a \\neq 0\\) and \\(a^Tx \\leq a^T x_0\\) for all \\(x\\in C\\) Theorem 1.2 If \\(C\\) is convex, then there exists a supporting hyperplane at every boundary point of \\(C\\). 1.5 Dual cones and generalized inequalities Definition 1.4 dual cone of a cone \\(K\\) \\[\\begin{equation} \\tag{1.8} K^* = \\{ y | y^T x \\geq 0 \\text{ for all } x\\in K\\} \\end{equation}\\] Examples: \\(R^n_+\\): \\(R^n_+\\) \\(K = S_+^n\\): \\(S_+^n\\) (\\(&lt;x,y&gt; = Tr(XY)\\)) \\(K = \\{ (x, t) | \\| x \\|_2 \\leq t \\}\\): \\(K = \\{ (x, t) | \\| x \\|_2 \\leq t \\}\\) \\(K = \\{ (x, t) | \\| x \\|_1 \\leq t \\}\\): \\(K = \\{ (x, t) | \\| x \\|_\\infty \\leq t \\}\\) Note that \\((K^*)^* = K\\) only if \\(K\\) is proper. Also, since dual cones of proper cones are proper, this defines generalized inequalities: \\[\\begin{equation*} y \\succeq_K^* 0 \\Longleftrightarrow y^Tx \\geq 0 \\text{ for all } x\\succeq_K 0 \\end{equation*}\\] 1.6 Minimum and minimal elements via dual inequalities Theorem 1.3 \\(x\\in S\\) is the minimum element of \\(S\\) with respect to \\(\\preceq_K\\) iff for all \\(\\lambda \\succ_{K^*} 0\\), \\(x\\) is the unique minimizer of \\(\\lambda^T z\\) over \\(S\\). Theorem 1.4 \\(x\\in S\\) is a minimal element of \\(S\\) with respect to \\(\\preceq_K\\) if \\(x\\) minimizes \\(\\lambda^Tz\\) over \\(S\\) for some \\(\\lambda \\succ_{K^*} 0\\) "],["convex-functions.html", "Chapter 2 Convex functions 2.1 Definition 2.2 Examples 2.3 Examples on \\(R^n\\) and \\(R^{m\\times n}\\) 2.4 Restriction of a convex function to a line 2.5 Extension of a function outside of its domain 2.6 1st-order condition 2.7 2nd-order conditions 2.8 Examples 2.9 Epigraph and sublevel set 2.10 Jensen’s inequality 2.11 Positive weighted sum and composition with affine functions 2.12 Pointwise maximum 2.13 Pointwise supremum 2.14 Composition 2.15 Vector Composition 2.16 Pointwise infinium 2.17 Perspective function 2.18 The conjugate function 2.19 Dummy", " Chapter 2 Convex functions 2.1 Definition Definition 2.1 \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) is convex if \\(\\mathbf{dom} f\\) is a convex set and \\[\\begin{equation} \\tag{2.1} f(\\underbrace{\\theta x + (1-\\theta) y}_\\text{convex mixture}) \\leq \\theta f(x) + (1-\\theta) f(y) \\end{equation}\\] for all \\(x, y \\in \\mathbf{dom} f\\), \\(0\\leq \\theta \\leq 1\\). i.e. “Any chord is above function.” \\(f\\) is concave if \\(-f\\) is convex. 2.2 Examples convex: affine functions \\(ax+b\\) exponential \\(e^{ax}\\) powers: \\(x^a\\) on \\(R_{++}\\), for \\(a\\geq 1\\) or \\(a\\leq 0\\) powers of absolute value: \\(| x |^p\\) on \\(R\\), for \\(p\\geq 1\\) negative entropy: \\(x \\log x\\) on \\(R_{++}\\) concave: affine functions \\(ax+b\\) (lol!) powers: \\(x^a\\) on \\(R_{++}\\), for \\(0 \\leq a \\leq 1\\) log: \\(\\log x\\) on \\(R_{++}\\) 2.3 Examples on \\(R^n\\) and \\(R^{m\\times n}\\) Affine functions are both convex and concave; all norms are convex. 2.3.1 On \\(R^n\\) affine: \\(a^Tx+b\\) norms: \\(\\| x \\|_p = (\\sum_{i=1}^n | x_i | ^p )^{1/p}\\) for \\(p\\geq 1\\); \\(\\| x\\|_\\infty = max_k |x_k|\\) 2.3.2 On \\(R^{m\\times n}\\) (matrices) Affine: \\[\\begin{align} \\tag{2.2} f(X) &amp; = &amp; \\langle A, X \\rangle + b \\\\ &amp; = &amp; \\mathbf{tr}(A^TX) + b \\end{align}\\] Spectral norm (maximum singular value norm): \\[\\begin{align} \\tag{2.3} f(X)= \\| X \\|_2 = \\sigma_{\\mathrm{max}}(X) = (\\lambda_{\\mathrm{max}}(X^TX))^{1/2} \\end{align}\\] 2.4 Restriction of a convex function to a line Theorem 2.1 \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) is convex iff the function \\(g: \\mathbf R \\rightarrow \\mathbf R\\) \\[\\begin{equation*} g(t) = f(x + tv), \\mathbf{dom} g = \\{ t | x + tv \\in \\mathbf{dom} f \\} \\end{equation*}\\] is convex in \\(t\\) for any \\(x \\in \\mathbf{dom} f\\), \\(v \\in \\mathbf R^n\\). This fact (2.1) is very important when speculating :) and proving whether a function is convex or concave. 2.5 Extension of a function outside of its domain extended-value extension \\(\\bar f\\) of \\(f\\) is \\[\\begin{align} \\tag{2.4} \\bar f = f(x), x\\in \\mathbf{dom} f\\\\ \\bar f = \\infty, x \\notin \\mathbf{dom} f \\end{align}\\] This usually simplifies the notations when doing analysis. 2.6 1st-order condition Theorem 2.2 differentiable \\(f\\) with convex domain is convex iff \\[\\begin{equation*} f(y) \\geq f(x) + \\nabla f(x)^T (y-x) \\text{ for all } x,y \\in \\mathbf{dom} f \\end{equation*}\\] i.e. the 1st-order Taylor expansion (local) is the global under-estimator for \\(f\\). 2.7 2nd-order conditions Theorem 2.3 twice differentiable \\(f\\) with convex domain is convex iff \\[\\begin{equation*} \\nabla^2 f(x) \\succeq 0 \\text{ for all } x \\in \\mathbf{dom} f. \\end{equation*}\\] If (not iff) \\(\\nabla^2 f(x) \\succ 0\\), then \\(f\\) is strictly convex. 2.8 Examples Example 2.1 quadratic function: \\(f(x) = (1/2)x^TPx+ q^T x + r\\) (with \\(P\\in S^n\\)) \\[\\begin{equation*} \\nabla f(x) = Px+q, \\nabla^2 f(x) = P \\end{equation*}\\] convex if \\(P\\succeq 0\\). Example 2.2 least squares objective: \\(f(x) = \\|Ax-b\\|^2_2\\) \\[\\begin{equation*} \\nabla f(x) = 2A^T(Ax-b), \\nabla^2 f(x) = 2A^TA, \\end{equation*}\\] is convex. Example 2.3 quadratic-over-linear: \\(f(x,y) = x^2/y\\) \\[\\begin{equation*} \\nabla^2 f(x,y) = \\frac 2 {y^3}\\begin{bmatrix} y \\\\ -x \\end{bmatrix} \\begin{bmatrix} y \\\\ -x \\end{bmatrix}^T \\succeq 0, \\end{equation*}\\] is convex if \\(y &gt; 0\\). Example 2.4 log-sum-exp (softmax): \\(f(x) = \\log \\sum^n_{k=1} e^{x_k}\\), \\(z_k = \\exp x_k\\) \\[\\begin{equation*} \\nabla^2 f(x) = \\text{&lt;horrible mess&gt;} = \\frac 1 {\\mathbf{1}^{T}z} \\mathbf{diag}(z) - \\frac 1 {(\\mathbf{1}^T z)^2}zz^T, \\end{equation*}\\] is convex. Proof is omitted, but very important! 2.9 Epigraph and sublevel set Definition 2.2 \\(\\alpha\\)-sublevel set of \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\): \\[\\begin{equation*} C_\\alpha = \\{ x\\in \\mathbf{dom} f | f(x) \\leq \\alpha \\} \\end{equation*}\\] Sublevel sets of convex functions are convex (but not in reverse). Definition 2.3 epigraph of \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\): \\[\\begin{equation*} \\mathbf{epi} f = \\{ (x,t) \\in R^{n+1} | x\\in \\mathbf{dom} f, f(x) \\leq t \\} \\end{equation*}\\] \\(f\\) is convex iff \\(\\mathbf{epi} f\\) is a convex set. 2.10 Jensen’s inequality An extension to (2.1) is the Jensen’s inequality for expectations: \\[\\begin{equation} \\tag{2.5} f(\\mathbf E z) \\leq \\mathbf E f(z) \\end{equation}\\] for any random variable \\(z\\). For a discrete distribution, (2.1) reduces to equality \\(P(B) = 1 - P(A)\\). 2.11 Positive weighted sum and composition with affine functions Note that if \\(f\\) is convex, \\(f(Ax+b)\\) is also convex. Examples: log barrier: \\[\\begin{equation*} f(x) = - \\sum^m \\log(b_i - a_i^Tx), \\mathbf{dom} f = \\{ x| a_i^Tx &lt; b_i\\} \\end{equation*}\\] 2.12 Pointwise maximum Proposition 2.1 if \\(f_1,\\dots, f_m\\) are convex, then \\(f(x) = max\\{f_1(x), \\dots, f_m(x)\\}\\) is convex. Example 2.5 piecewise-linear: \\(\\max_{i=1,\\dots,m}(a_i^Tx+b_i)\\) is convex. Example 2.6 sum of \\(r\\) largest components of \\(x\\in R^n\\): \\[\\begin{equation*} f(x) = x_{[1]} + \\cdots + x_{[r]} \\end{equation*}\\] is convex. Note: \\(f(x)\\) can be seen as the max of \\(C_n^r\\) sums. 2.13 Pointwise supremum Proposition 2.2 if \\(f(x,y)\\) is convex in \\(x\\) for each \\(y \\in \\mathcal A\\), then \\[\\begin{equation} \\tag{2.6} g(x) = \\sup_{y\\in \\mathcal A} f(x, y) \\end{equation}\\] is convex. Example 2.7 support function of a set \\(C\\): \\(S_C(x) = \\sup_{y\\in C}y^Tx\\) Example 2.8 max eigenvalue of symmetric matrix: for \\(X\\in S^n\\), \\[\\begin{equation} \\tag{2.7} \\lambda_{max}(X) = \\sup_{\\|y\\|_2=1}\\underbrace{y^TXy}_{\\text{linear in }X} \\end{equation}\\] is convex. 2.14 Composition \\[\\begin{equation} \\tag{2.8} f(x) = h(g(x)) \\end{equation}\\] \\(f\\) is convex if \\(g\\) convex, \\(\\tilde h\\) convex and nondecreasing \\(g\\) concave, \\(\\tilde h\\) convex and nonincreasing Note that \\(\\tilde h\\) is the extended value function so be careful! 2.15 Vector Composition Similar to 2.14 2.16 Pointwise infinium Proposition 2.3 if \\(f(x,y)\\) is convex in \\((x,y)\\) and \\(C\\) is a convex set, then \\[\\begin{equation} \\tag{2.9} g(x) = \\inf_{y\\in \\mathcal C} f(x, y) \\end{equation}\\] is convex. Example 2.9 Schur complement \\(f(x,y) = x^TAx + 2x^TBy + y^TCy\\) with \\[\\begin{equation} \\tag{2.10} \\begin{bmatrix} A &amp; B \\\\ B^T &amp; C \\end{bmatrix} \\succeq 0, C \\succ 0 \\end{equation}\\] minimizing over \\(y\\) gives \\(g(x) = \\inf_y f(x,y) = x^T(A-BC^{-1}B^T)x\\) (Schur complement). \\(g\\) is convex, thus the Schur complement \\(A-BC^{-1}B^T \\succeq 0\\). Example 2.10 distance to a set \\[\\begin{equation} \\tag{2.11} \\mathbf{dist}(x, S) = \\inf_{y\\in S} \\|x-y\\| \\end{equation}\\] is convex if \\(S\\) is convex. 2.17 Perspective function Definition 2.4 the perspective of a function \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) is the function \\(g: \\mathbf R^n \\times R \\rightarrow \\mathbf R\\), \\[\\begin{equation} g(x,t) = tf(x/t), \\mathbf{dom} g = \\{ (x,t) | x/t \\in \\mathbf{dom} f, t&gt;0 \\} \\end{equation}\\] \\(g\\) is convex if f is convex. Example 2.11 \\(f(x) = x^Tx\\) is convex, then \\(g(x, t) = x^Tx/t\\) is convex for \\(t&gt;0\\). Example 2.12 negative log \\(f(x) = -\\log x\\) is convex, thus the relative entropy \\[\\begin{equation} \\tag{2.12} g(x,t) = t\\log t - t \\log x \\end{equation}\\] is convex on \\(\\mathbf R^2_{++}\\). (This is related to KL divergence) Example 2.13 if \\(f\\) convex, then \\[\\begin{equation} \\tag{2.13} g(x) = (c^Tx+d) f \\left ( (Ax+b)/(c^Tx+d) \\right ) \\end{equation}\\] is convex on \\(\\{ x | c^Tx+d &gt; 0 , (Ax+b)/(c^Tx+d) \\in \\mathbf{dom} f \\}\\) 2.18 The conjugate function Definition 2.5 the conjugate of a function \\(f\\) is \\[\\begin{equation} \\tag{2.14} f^*(y) = \\sup_{x\\in \\mathbf{dom} f}(y^Tx-f(x)) \\end{equation}\\] Note that \\(f^*\\) is convex regardless of what \\(f\\) is. And an interesting fact is that \\[\\begin{equation} \\mathbf{epi} (f^{env}) = \\mathbf{conv}(\\mathbf{epi} f) \\end{equation}\\] (convex envelope) 2.19 Dummy Lorem Lorem Lorem Lorem \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) \\(f: \\mathbf R^n \\rightarrow \\mathbf R\\) "],["linear-programs.html", "Chapter 3 Linear Programs 3.1 Piecewise linear minimization 3.2 Chebyshev center of a polyhedron", " Chapter 3 Linear Programs Here are some famous LPs: 3.1 Piecewise linear minimization \\[\\begin{align} \\tag{3.1} \\text{minimize}&amp; &amp; \\max _{i=1,\\dots,m}(a_i^Tx+b_i) \\end{align}\\] is equivalent to the LP \\[\\begin{align} \\tag{3.2} \\text{minimize}&amp; &amp; t &amp; &amp; \\\\ \\text{subject to} &amp; &amp; a_i^Tx+b_i \\leq t &amp; , &amp; i=1,\\dots,m. \\end{align}\\] Note that PWL can approximate any function, so this can be very useful! 3.2 Chebyshev center of a polyhedron The Chebyshev center of a polyhedron \\[\\begin{equation} \\tag{3.3} \\mathcal P = \\{ x|a_i^Tx &lt; b_i \\} \\end{equation}\\] is defined to be the center of a maximum sphere \\[\\begin{equation} \\tag{3.4} \\mathcal B = \\{ x_c + u | \\| u \\|_2 \\leq r \\} \\end{equation}\\] inside the polyhedron (note this can be multiple such points). Furthermore, \\(a_i^Tx \\leq b_i\\) for all \\(x \\in \\mathcal B\\) if and only if \\[\\begin{equation} \\tag{3.5} \\sup\\{ a_i^T (x_c + u) | \\| u \\|_2 \\leq r\\} = a_i^T x_c + r \\| a_i \\|_2 \\leq b_i \\end{equation}\\] This is actually an LP: \\[\\begin{align} \\tag{3.6} \\text{maximize}&amp; &amp; r &amp; &amp; \\\\ \\text{subject to} &amp; &amp; a_i^Tx_c + \\|a_i\\|_2 r \\leq b_i, &amp; &amp; i=1,\\dots,m \\end{align}\\] "],["interior-point-methods.html", "Chapter 4 Interior Point Methods 4.1 KKT conditions 4.2 Logarithmic barrier function and central path", " Chapter 4 Interior Point Methods Inequality constrained minimization problems can be written as \\[\\begin{align} \\tag{4.1} \\text{minimize}&amp; &amp; f_0(x) &amp; &amp; \\\\ \\text{subject to}&amp; &amp; f_i(x) \\leq 0, &amp; &amp; i = 1,\\dots,m \\\\ &amp; &amp; Ax=b &amp; \\end{align}\\] where \\(f_0,\\dots,f_m:\\mathbf{R}^n\\rightarrow \\mathbf R^n\\) are convex and \\(\\mathcal{C}^2\\) differentiable, and \\(A\\in \\mathbf R^{p\\times n}\\) with \\(\\mathbf{rank} A = p &lt; n\\) (Boyd and Vandenberghe 2004). Assuming that the solution \\(x^*\\) exists with the optimum value \\(p^* = f_0(x^*)\\). 4.1 KKT conditions \\[\\begin{align} \\tag{4.2} Ax^* = b, f_i(x^*) &amp; \\leq &amp; 0 &amp;, &amp; i=1,\\dots, m \\\\ \\lambda^* &amp; \\leq &amp; 0 &amp; &amp; \\\\ \\nabla f_0(x^*) + \\sum_{i=1}^{m} \\lambda^*_i \\nabla f_i(x^*) + A^Tv^* &amp; = &amp; 0 &amp; &amp; \\\\ \\lambda^*_i f_i(x^*) &amp; = &amp; 0 &amp;, &amp; i=1,\\dots,m \\end{align}\\] The KKT conditions (4.2) means, respectively: All constraints are satisified at \\(x^*\\); Corresponding Lagrange multiplier is sufficiently larger than zero; The gradient of the Lagrangian at \\(x^*\\) is 0; Inequality constraints are either inactive or on the edge. 4.2 Logarithmic barrier function and central path The aim is to approximate (4.1) as an equality-constrained problem and apply Newton’s method. This proposes the following formulation: \\[\\begin{align} \\tag{4.3} \\text{minimize}&amp; &amp; f_0(x) + \\sum_{i=1}^m I_{-}(f_i(x)) \\\\ \\text{subject to}&amp; &amp; Ax = b, \\end{align}\\] where \\(I_{-} : \\mathrm R \\rightarrow \\mathrm R\\) is the indicator function \\[\\begin{equation} \\tag{4.4} I_{-}(u) = \\begin{cases} 0 &amp; u\\leq 0 \\\\ \\infty &amp; u &gt; 0. \\end{cases} \\end{equation}\\] However, it is obvious that (4.3) is non-differentiable, so we cannot optimize it with Newton’s method. 4.2.1 Logarithmic barrier We can replace \\(I_{-}\\) (4.4) with another function: \\[\\begin{equation} \\tag{4.5} \\hat I_{-}(u) = -(1/t) \\log (-u) \\end{equation}\\] Figure 4.1: Logarithmic barrier This will convert (4.3) "],["references.html", "References", " References Boyd, Stephen, and Lieven Vandenberghe. 2004. “Interior-Point Methods.” In Convex Optimization, 561–630. Cambridge University Press. https://doi.org/10.1017/CBO9780511804441.012. "]]
