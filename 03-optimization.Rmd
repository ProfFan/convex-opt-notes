# Convex Optimization

## Optimization in standard form

\begin{align}
    (\#eq:generic-optimization)
    \text{minimize}& & f_0(x) & & \\
    \text{subject to} & & f_i(x) \leq 0 & , & i=1,\dots,m \\
    & & h_i(x) = 0 & , & i=1,\dots,p
\end{align}

A standard form of describing optimization problems is \@ref(eq:generic-optimization), where

* $x\in R^n$ is the optimization variable
* $f_0: R^n \rightarrow R$ is the objective or cost function
* $f_i: R^n \rightarrow R$ are the inequality constraints
* $h_i: R^n \rightarrow R$ are the equality constraints

For such a description, the optimal value is defined as
\begin{equation}
    (\#eq:optimal-value-generic)
    p^* = \inf \{f_0(x) | f_i(x)\leq 0, h_i(x) = 0 \}.
\end{equation}

* When the problem is infeasible, $p^*=\infty$,
* when it's unbounded below, $p^* = -\infty$.

## Optimality and local optimality

* **feasible**: in domain and satisfy all constraints
* **optimal**: $f_0(x) = p^*$
* **locally optimal**: exists $R>0$ that $x$ is optimal under $\| z-x \|_2 \leq R$.

## Implicit constraints

(\@ref(eq:generic-optimization)) has an **implicit constraint**
\begin{equation}
    (\#eq:implicit-constraint)
    x\in \mathcal D = \bigcap_{i=0}^m \mathbf{dom} f_i \cup \bigcap_{i=1}^p \mathbf{dom} h_i
\end{equation}

* $\mathcal D$ is called the **domain** of the problem
* a problem is **unconstrained** if it has no *explict* constraints ($m=p=0$)


```{example}
\begin{align}
\text{minimize} & & f_0 = -\sum_{i=1}^k \log (b_i-a_i^Tx) 
\end{align}
is an unconstrained problem with *implicit constraint* of $a_i^Tx<b_i$.
```


## Feasibility problem

\begin{align}
    (\#eq:generic-feasibility)
    \text{find}& & x & & \\
    \text{subject to} & & f_i(x) \leq 0 & , & i=1,\dots,m \\
    & & h_i(x) = 0 & , & i=1,\dots,p
\end{align}
can be actually written as
\begin{align}
    (\#eq:generic-feasibility-as-opt)
    \text{minimize}& & 0 & & \\
    \text{subject to} & & f_i(x) \leq 0 & , & i=1,\dots,m \\
    & & h_i(x) = 0 & , & i=1,\dots,p
\end{align}

* $p^*=0$ if constraints are feasible
* $p^*=\infty$ otherwise ($\inf \varnothing = \infty$)

## Convex optimization

\begin{align}
    (\#eq:convex-optimization)
    \text{minimize}& & f_0(x) & & \\
    \text{subject to} & & f_i(x) \leq 0 & , & i=1,\dots,m \\
    & & Ax=b & & 
\end{align}

Boyd defines convex optimization to be:

* $f_i$ are all convex, equality constraints are all affine.
* also can define quasiconvex problems where $f_0$ is quasiconvex, and $f_i (i>0)$ are convex.

Note that the feasible set of a convex optimization problem is convex.

```{example}
\begin{align}
    \text{minimize}& & f_0(x)=x_1^2+x_2^2 & \\
    \text{subject to} & & f_i(x)=x_1/(1+x_2^2) \leq 0 &  \\
    & & h_1(x) = (x_1 + x_2)^2=0 & 
\end{align}

* $f_0$ is convex, feasible set $\{ x_1 = -x_2 \}$ is also convex
* *however* not a convex problem according to \@ref(eq:convex-optimization)
* we can rewrite to equivalent (not identical) convex problem
  \begin{align}
    \text{minimize}& & f_0(x)=x_1^2+x_2^2 & \\
    \text{subject to} & & x_1 \leq 0 &  \\
    & & h_1(x) = x_1 + x_2=0 & 
  \end{align}

```

## Optimality in convex optimization

```{theorem, global-optimality-convex}
Any local optimal point is optimal globally for a convex problem.
```

One can easily prove (\@ref(thm:global-optimality-convex)) by contradiction.

### Optimality criterion for differentiable $f_0$

```{theorem, optimal-criterion-differentiable}
$x$ is optimal **iff** $x$ is feasible and
\begin{align}
    (\#eq:optimal-criterion-differentiable)
    \nabla f(x)^T (y-x) \geq 0 & \text{ for all feasible }y
\end{align}
```

We have seen this before in \@ref(thm:first-order-condition).

```{example}
unconstrained problem: $\nabla f(x) = 0$
```

```{example, equality-constrained}
equality constrained problem (only $Ax=b$):
$x$ is optimal **iff** there exists $\nu$ such that
$x\in \mathbf{dom} f_0$, $Ax=b$, $\nabla f_0(x) + A^T\nu=0$
```

```{proof}
Assume $x$ is the optimum.
$\nabla f_0(x)^T (z-x) \geq 0$ for all $z$, $Az=b$. Also $Ax=b$.

Thus $z-x \in \mathrm{Null}(A)$, i.e. $\nabla f_0(x) \perp \mathrm{Null}(A)$.

Consequently, $f_0(x) \in \operatorname{im} (A^T)$,

Thus $\nabla f_0(x) = A^Tu$, $\nabla f_0(x) + A^T(-u) = 0$.
```

```{example, minimization-nonneg-orthant}
minimization over non-negative orthant ($R_{++}^n$):

\begin{align}
    \text{minimize}& & f_0(x) & \\
    \text{subject to} & & x \succeq 0 &
\end{align}

$x$ is optimal **iff**
\begin{align}
    x\in \mathrm{dom}f_0, & x\succeq 0, & \begin{cases}
        \nabla f_0(x)_i \geq 0, &x_i = 0 \\
        \nabla f_0(x)_i = 0, &x_i > 0 \\
    \end{cases}
\end{align}
```

```{proof}
Assume $x$ is the optimum.
$\nabla f_0(x)^T (z-x) \geq 0$ for all $z\geq 0$.

Plugging in $z=0$, $\nabla f(x)^Tx \leq 0$.

If any of the components ($i$-th) of $\nabla f(x)$ is negative, then we can take $z_i\rightarrow \infty$, and LHS will be negative, thus we know that $\nabla f(x) \geq 0$.

Since $\nabla f(x)^Tx \leq 0$ and $\nabla f(x) \geq 0$, we know that $\nabla f(x)_i x_i = 0$ for all $i$ (**complementarity**).

```

## Equivalent convex problems

This section is full of trickery that you will regret if you don't know.

### eliminating equality constraints

\begin{align}
    \text{minimize}& & f_0(x) & & \\
    \text{subject to} & & f_i(x) \leq 0 & , & i=1,\dots,m \\
    & & Ax=b & & 
\end{align}
is equivalent to
\begin{align}
    (\#eq:convex-optimization-equivalent)
    \text{minimize (over z)}& & f_0(Fz+x_0) & & \\
    \text{subject to} & & f_i(Fz+x_0) \leq 0 & , & i=1,\dots,m \\
\end{align}
where $F$ and $x_0$ are such that
\begin{align}
    (\#eq:eliminate-equality-constraint)
    Ax=b & \Longleftrightarrow & x=Fz+x_0 \text{ for some }z
\end{align}

How to get $F$ and $x_0$? Well $F$ spans the null space of $A$, and $x_0$ is any particular solution of $Ax=b$.

Note affine operation preserves convexity, so the eliminated problem is also convex.

Boyd: An intriguing fact is that in reality not only it's bad to eliminate constraints, it's often a good idea to add (un-eliminate) equality constraints!

Boyd: Convex problems (by definition here) only has affine equality constraints.

### Introducing equality constraints

\begin{align}
    \text{minimize}& & f_0(A_0x+b_0) & & \\
    \text{subject to} & & f_i(A_0x+b_0) \leq 0 & , & i=1,\dots,m \\
\end{align}
is equivalent to
\begin{align}
    \text{minimize over $x$, $y_i$}& & f_0(y_0) & & \\
    \text{subject to} & & f_i(y_i) \leq 0 & , & i=1,\dots,m \\
    & & y_i=A_ix+b_i & & i=1,\dots,m
\end{align}

### Introducing slack variables for linear inequalities

\begin{align}
    \text{minimize}& & f_0(x) & & \\
    \text{subject to} & & a_i^Tx \leq b_i & , & i=1,\dots,m \\
\end{align}
is equivalent to
is equivalent to
\begin{align}
    \text{minimize over $x$, $s$}& & f_0(x) & & \\
    \text{subject to} & & a_i^Tx+s_i=b_i & , & i=1,\dots,m \\
    & & -s_i \leq 0 & & i=1,\dots,m
\end{align}

Boyd: this looks stupid but is actually useful.

### The epigraph form (trick)

The standard form \@ref(eq:convex-optimization) is equivalent to
\begin{align}
    \text{minimize over $x$, $t$}& & t & & \\
    \text{subject to} & & f_0(x) - t \leq 0 &  &  \\
                      & & f_i(x) \leq 0 & , & i=1,\dots,m \\
    & & Ax=b & & 
\end{align}

Note that this actually converts **any** objective to a **linear** objective!

### Minimizing over some variables

\begin{align}
    \text{minimize}& & f_0(x_1, x_2) & & \\
    \text{subject to} & & f_i(x_1)  \leq 0 &  & i=1,\dots,m
\end{align}
is equivalent to
\begin{align}
    \text{minimize}& & \tilde f_0(x_1) & & \\
    \text{subject to} & & f_i(x_1)  \leq 0 &  & i=1,\dots,m
\end{align}
where $\tilde f_0 (x_1) = \inf_{x_2} f_0(x_1, x_2)$

a.k.a. dynamic programming preserves convexity of a problem.

## Quasiconvex optimization

\begin{align}
    (\#eq:quasiconvex-optimization)
    \text{minimize}& & f_0(x) & & \\
    \text{subject to} & & f_i(x) \leq 0 & , & i=1,\dots,m \\
    & & Ax=b & & 
\end{align}
where $f_0$ is quasiconvex and $f_1, \dots, f_m$ convex.

Quasiconvex functions can have locally optimal points that are not globally optimal.

### Convex representation of its sublevel sets

```{definition, convex-repr-quasiconvex}
Convex representation of sublevel sets of $f_0$:

If $f_0$ is quasiconvex, there exists a family of functions $\phi_t$ such that:

* $\phi_t(x)$ is convex in $x$ for fixed $t$
* $t$-sublevel set of f_0 is 0-sublevel set of $\phi_t$, *i.e.*
  \begin{equation}
      f_0(x)\leq t \Longleftrightarrow \phi_t(t) \leq 0
  \end{equation}

```

```{example, fractional-constraint-rewrite}
\begin{equation*}
    f_0(x) = \frac{p(x)}{q(x)}
\end{equation*}
with $p\geq 0$ convex and $q>0$ concave, $f_0(x)$ is quasiconvex on $\mathrm{dom} f_0$.

Thus we can take $\phi_t(x) = p(x) - tq(x)$.
```

```{proof}
Sublevel set: $f_0(x)=p(x)/q(x)\leq t$. Thus
\begin{align}
    p(x)\leq tq(x) \\
    p(x) - tq(x) \leq 0
\end{align}

Obviously $\phi_t(x)$ is convex.
```

### Quasiconvex optimization via convex feasibility problems

Now that we have $\phi_t$, what can we do to solve the original problem?

```{lemma, quasiconvex-opt-via-convex-feasibility}
Quasiconvex optimization via convex feasibility problems:
\begin{align}
    \phi_t(x) \leq 0,& & f_i(x) \leq 0, & & i=1,\dots, m, & & Ax=b
\end{align}

* for any fixed $t$, a convex feasibility problem in $x$.
* if feasible, then we know $t\geq p^*$, and conversely $t<p^*$ otherwise.
```

This is an oracle that enables a bisection method for quasiconvex optimization.

This method bisects the given interval $[u, l]$, which requires exactly $\lceil \log_2 ((u-l)/\epsilon)\rceil$ iterations, where $\epsilon$ is the accuracy.

## Linear Programs (LP)

```{definition, lp}
**Linear Program (LP)** is defined as
\begin{align}
    (\#eq:lp-generic)
    \text{minimize}& & c^Tx+d & & \\
    \text{subject to} & & Gx\preceq h & , &  \\
    & & Ax=b & & 
\end{align}

```

This is convex problem with affine objective and constraints. The feasible set is a polyhedron.

Note since the level set of an affine function is a hyperplane, what you need to do is actually just move the hyperplane as far as possible while intersecting with the polyhedron.

Note that even for a small LP with 10s of variables, the number of vertices of the polyhedron will be astronomical (quote Boyd).

### Examples

Here are some famous LPs:

#### Diet problem

Minimize cost of food $c$ given nutrient requirements $b$:
\begin{align}
  (\#eq:diet-problem)
  \text{minimize}& & c^Tx & & \\
    \text{subject to} & & Ax\succeq  b & , & x\succeq 0
\end{align}

#### Piecewise linear minimization

\begin{align}
    (\#eq:piecewise-linear-min)
    \text{minimize}& & \max _{i=1,\dots,m}(a_i^Tx+b_i)
\end{align}
is equivalent to the LP
\begin{align}
    (\#eq:piecewise-linear-min-lp)
    \text{minimize}& & t & & \\
    \text{subject to} & & a_i^Tx+b_i \leq t & , & i=1,\dots,m.
\end{align}

Note that PWL can approximate any convex function, so this can be very useful!

#### Chebyshev center of a polyhedron

The Chebyshev center of a polyhedron 
\begin{equation}
  (\#eq:chebyshev-point-poly)
  \mathcal P = \{ x|a_i^Tx \leq b_i \}
\end{equation}
is defined to be the center of a maximum sphere
\begin{equation}
  (\#eq:chebyshev-point-sphere)
  \mathcal B = \{ x_c + u | \| u \|_2 \leq r \}
\end{equation}
inside the polyhedron (note this can be multiple such points).

This is an interesting problem that, seeing a sphere will bring you immediately that this problem is nonlinear. However it is not. 

Note that, $a_i^Tx \leq b_i$ for all $x \in \mathcal B$ if and only if
\begin{equation}
  (\#eq:chebyshev-point)
  \sup\{ a_i^T (x_c + u) | \| u \|_2 \leq r\}  = a_i^T x_c + r \| a_i \|_2 \leq b_i
\end{equation}
by substituting $x = x_c+u$.

Also note that this is linear in $x_c$ and $r$, so it is actually an LP:
\begin{align}
    (\#eq:chebyshev-point-lp)
    \text{maximize}& & r &  & \\
    \text{subject to} & & a_i^Tx_c + \|a_i\|_2 r \leq b_i, &  & i=1,\dots,m
\end{align}

This problem is very related to the yield maximization problem in industry production (maximizing the probability of producing an item that falls in all tolerances).

This can also be generalized to other norms (where $\| \|_*$ is the dual norm of $\| \|$):
\begin{equation}
  (\#eq:chebyshev-point-general)
  \sup\{ a_i^T (x_c + u) | \| u \| \leq r\}  = a_i^T x_c + r \| a_i \|_* \leq b_i
\end{equation}

### Linear-fractional program

```{definition, lp-fractional}
**Linear-fractional program** is defined as
\begin{align}
    (\#eq:lp-fractional)
    \text{minimize}& & f_0(x) & & \\
    \text{subject to} & & Gx\preceq h & , &  \\
    & & Ax=b & & 
\end{align}
where $$f_0(x)=\frac{c^Tx+d}{e^Tx+f}$$, $\mathrm{dom} f_0 = \{ e^Tx+f > 0 \}$.

```

This is quasiconvex (quasilinear) as said in (\@ref(quasiconvexity-examples)).

When the feasible set is not empty, we can transform \@ref(def:lp-fractional) to equivalent LP
\begin{align}
    (\#eq:lp-fractional-equiv)
    \text{minimize}& & c^Ty+dz & & \\
    \text{subject to} & & Gy-hz\preceq 0 & , &  \\
    & & Ay-bz = 0 & & \\
    & & e^Ty + fz = 1 & & \\
    & & z \geq 0 & &
\end{align}

```{solution}
(*My solution*) Since $f_0(x)=\frac{c^Tx+d}{e^Tx+f}$, we can introduce additional variables $y$ and $z$ such that
\begin{align}
    z = 1/(e^Tx + f), & & f_0(x,z) = (c^Tx +d)z \\
    y = xz, & & f_0(y, z) = c^Ty + dz
\end{align}

Now the old set of constraints becomes fractionals $G(y/z)\preceq h$ and $A(y/z) = b$. We then apply (\@ref(exm:fractional-constraint-rewrite)), and it's easy to show the result after that.

```

```{solution}
(*Boyd's solution*) Since $f_0(x)=\frac{c^Tx+d}{e^Tx+f}$, note that adding a renormalization constant $z$ does not change the solution of the problem:
\begin{align}
    \text{minimize}& & \frac{c^Tx+dz}{e^Tx+fz} & & \\
    \text{subject to} & & Gx\preceq hz & , &  \\
    & & Ax=bz & & \\
    & & z \geq 0 & &
\end{align}
```

### Generalized linear-fractional program

```{definition, gen-lp-fractional}
**Generalized linear-fractional program** is defined as
\begin{align}
    (\#eq:gen-lp-fractional)
    f_0(x)=\max_{i=1,\dots, r} \frac{c^Tx+d}{e^Tx+f}
\end{align}
where $\mathrm{dom} f_0 = \{ e^Tx+f > 0 \}$.

```

This is a quasiconvex but not convertible to LP. Can only be solved by bisection.

```{example}
*von Neumann* model of a growing economy
\begin{align}
    \text{minimize} & & \min_{i=1,\dots,n} x_i^+ / x_i &\\
    \text{subject to} & & x^+ \succeq 0, Bx^+ \preceq Ax & \\
    & & x \geq 0 &
\end{align}

* $x,x^+ \in R^n$: activity levels of n sectors in current and next period
* $(Ax)_i, (Bx^+)_i$: produced and consumed amounts of good $i$
* $x^+_i/x_i$: growth rate of section $i$
```

## Quadratic programs (QP)

```{definition, qp}
**Quadratic program (QP)** is defined as
\begin{align}
    (\#eq:qp-generic)
    \text{minimize}& & (1/2)x^TPx + q^Tx + r & & \\
    \text{subject to} & & Gx\preceq h & , &  \\
    & & Ax=b & & 
\end{align}
```

* $P \in S_+^n$, objective is convex quadratic
* We are currently in the 1950s (lol)

```{remark}
If $P$ is not positive semidefinite, then the problem becomes NP-hard!
```

### Examples

```{example}
**least-squares**:
\begin{equation*}
    \min . \| Ax-b \|^2_2
\end{equation*}
which has analytical solution (pseudoinverse) $x^* = A^\dagger b$.
You can also add linear constraints like $l\preceq x \preceq u$, and that is **just** a QP.
```

```{example}
**linear program with random cost**: (variant of the diet problem \@ref(eq:diet-problem))
\begin{align*}
    \min . & & \bar c^T x + \gamma x^T\Sigma x = \mathbb{E} c^Tx + \gamma \mathbb{var}(c^Tx) \\
    s.t. & & Gx \preceq h, Ax=b
\end{align*}
$\gamma > 0$ risk-factor.
```

## Quadratically constrained QP (QCQP)

```{definition, qcqp}
**Quadratic program (QP)** is defined as
\begin{align}
    (\#eq:qp-generic)
    \text{minimize}& & (1/2)x^TP_0x + q_0^Tx + r_0 & & \\
    \text{subject to} & & (1/2)x^TP_ix + q_i^Tx + r_i \leq 0& , & i=1,\dots,m \\
    & & Ax=b & & 
\end{align}
```

* $P_i \in S_+^n$, objective and constraints are convex quadratic
* if $P_i \in S_{++}^n$ problem becomes degenerative ellipisoids

## Second-order cone programming (SOCP)

```{definition, second-order-cone-program}
**Second-order cone program** is defined as
\begin{align}
    (\#eq:socp-generic)
    \text{minimize}& & f^Tx & & \\
    \text{subject to} & & \|A_ix+b_i\|_2\leq c_i^Tx+d_i & , & i=1,\dots,m \\
    & & Fx=g & & 
\end{align}
($A_i\in R^{n_i\times n}$, $F\in R^{p\times n}$)
```

* inequalities are called SOC constraints:
  \begin{equation*}
      (A_ix+b_i, c_i^Tx+d_i)\in \underbrace{\text{ second-order cone }}_\text{Lorenz cone}\text{ in } R^{n_i+1}
  \end{equation*}
* more general than QCQP and LP

```{remark}
If we rewrite $\|A_ix+b_i\|_2\leq c_i^Tx+d_i$ to the "standard" form, i.e.
$f_i(x) = \|A_ix+b_i\|_2 - c_i^Tx-d_i$, you will discover that $f_i$ is not differentiable at 0.

Note that obviously a lot of the $x_i$s can be zero, so this **does** matter!

```

## Robust linear programming

The parameters in an LP can be uncertain. Assuming that there are variations in $c$, $a_i$ and $b_i$.
\begin{align}
    \text{minimize}& & c^Tx & & \\
    \text{subject to} & & a_i^Tx\leq b_i & , &
\end{align}

### Approaches (Historical)

#### Deterministic model

\begin{align}
    (\#eq:robust-lp-deterministic)
    \text{minimize}& & c^Tx & & \\
    \text{subject to} & & a_i^Tx\leq b_i \text{ for all } a_i\in \mathcal E_i & , &
\end{align}

#### Stochastic model

\begin{align}
    (\#eq:robust-lp-stochastic)
    \text{minimize}& & c^Tx & & \\
    \text{subject to} & & \mathbf{prob}(a_i^Tx\leq b_i) \geq \eta & , &
\end{align}

### Deterministic approach via SOCP

We can choose an ellipsoid as $\mathcal E_i$:
\begin{align*}
    \mathcal E_i = \{ \bar a_i + P_i u | \| u \|_2 \leq 1 \} & & (\bar a_i \in R^n, P_i \in R^{n\times n})
\end{align*}

center is $\bar a_i$, semi-axes are determined by singular vectors of $P_i$.

Then \@ref(eq:robust-lp-deterministic) is equivalent to SOCP:
\begin{align}
    \text{minimize}& & c^Tx & & \\
    \text{subject to} & & \bar a_i^Tx + \| P_i^Tx \|_2 \leq b_i & , &
\end{align}

```{proof}
Substituting $a_i = \bar a_i + P_i u$, we have
\begin{equation*}
    (\bar a_i + P_iu)^Tx \leq b_i \\
    \sup_{\|u\|_2\leq 1} (\bar a_i + P_iu)^Tx \leq b_i \\
    \bar a_i^Tx + u^T(P_i^Tx) \leq b_i \\
    \bar a_i^Tx + \|P_i^Tx\|_2 \leq b_i
\end{equation*}

```

### Stochastic Approach via SOCP

Assuming $a_i \sim \mathcal N (\bar a_i, \Sigma_i)$. Then $a_i^Tx\sim \mathcal N (\bar a_i^Tx, x^T\Sigma_i x)$.
\begin{equation}
    \mathbf{prob}(a_i^Tx \leq b_i) = \Phi \left (  \frac{b_i - \bar a_i^Tx}{\| \Sigma_i^{1/2} \|_2} \right )
\end{equation}

Then, when $\eta \geq 1/2$ (risk aversion), the robust LP in \@ref(eq:robust-lp-stochastic) is equivalent to the SOCP
\begin{align}
    \text{minimize}& & c^Tx & & \\
    \text{subject to} & & \bar a_i^Tx + \Phi^{-1}(\eta)\| \Sigma_i^{1/2}x \|_2 \leq b_i & , &
\end{align}

## Geometric Program

```{definition, gp-monomial}
An monomial function (in GP!) is 
\begin{align*}
    f(x) = cx_i^{a_1}\dots x_n^{a_n},& & \mathbf{dom} f = R_{++}^n
\end{align*}

$c>0$; $a_i$ can be **any real number**.
```

```{definition, posynomial}
A posynomial function is the s um of (GP) monomials.
```

```{definition, geometric-program}
**Geometric Program**:
\begin{align}
    (\#eq:geometric-program)
    \text{minimize}& & f_0(x) & & \\
    \text{subject to} & & f_i(x)\leq 1 & , & \\
     & & h_i(x) = 1 & &
\end{align}
with $f_i$ posynomial, $h_i$ monomial.

```

GPs are not convex at all, e.g. minimizing $\sqrt x$!

### GP in convex form

Since everything here is positive, we can take the log by $y_i=\log x_i$, and log of all cost and constraints:

Monomials become
\begin{align*}
    \log f(e^{y_1}\dots e^{y_n}) = a^Ty+b ,& & b = \log c.
\end{align*}

Posynomials become
\begin{align*}
    \log f(e^{y_1}\dots e^{y_n}) = \log(\sum_{k=1}^K e^{a_k^Ty+b_k}) ,& & b_k = \log c_k.
\end{align*}

Now it can be transformed into a convex problem:

\begin{align}
    (\#eq:geometric-program-converted)
    \text{minimize}& & \log(\sum_{k=1}^K e^{a_{0k}^Ty+b_{0k}}) & & \\
    \text{subject to} & & \log(\sum_{k=1}^K e^{a_{ik}^Ty+b_{ik}})\leq 0 & , & \\
     & & Gy+d = 0 & &
\end{align}

### Minimizing spectral radius of a nonnegative matrix

```{definition, frobenius-norm}
**Perron-Frobenius eigenvalue** $\lambda_{pf}(A)$

* exists for (elementwise) positive $A\in R^{n\times n}$
* it's a real, positive eigenvalue of $A$, equal to spectral radius $\max_i | \lambda_i(A) |$
* determines asymptotic growth rate of $A^k$: $A^k \sim \lambda_{pf}^k$ as $k\rightarrow \infty$
* $\lambda_{pf}(A) = \inf \{ \lambda | Av \preceq \lambda v \text{ for some } v \succ 0 \}$
```

It turns out that we can minimize the spectral radius of a matrix of posynomials, i.e., minimize $\lambda_{pf}(A(x))$, where the elements of $A(x)$ are posynomials of $x$. The equivalent geometric program is

\begin{align}
    (\#eq:spectral-radius-geometric-program)
    \text{minimize}& & \lambda & & \\
    \text{subject to} & & \sum_{j=1}^n A(x)_{ij} v_j / (\lambda v_i) \leq 1 & , & i = 1,\dots, n
\end{align}

## Generalized inequality constraints



## Dummy

Lorem

Lorem
Lorem

Lorem

$f: \mathbf R^n \rightarrow \mathbf R$
$f: \mathbf R^n \rightarrow \mathbf R$
$f: \mathbf R^n \rightarrow \mathbf R$
$f: \mathbf R^n \rightarrow \mathbf R$